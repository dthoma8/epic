{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ES_Querying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, time, os\n",
    "import pandas as pd\n",
    "from elasticsearch import Elasticsearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = Elasticsearch([{'host': 'localhost', 'port': 9200}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define search query \n",
    "search_object = {'query': {'match': {'content': 'trump'}}}\n",
    "\n",
    "## Applying query term to indexed ES corpus\n",
    "sch_obj = es.search(index=\"corpus1\", body=search_object, size=1000) #Increase returns with size parameter, default 10\n",
    "#sch_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grabbing the list containing queries \n",
    "hits = sch_obj['hits']['hits']\n",
    "#hits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fetching hit's texts, sources, publications\n",
    "\n",
    "texts = [text['_source']['content'] for text in hits]\n",
    "publications = [ text['_source']['publication'] for text in hits]\n",
    "dates = [text['_source']['date'] for text in hits]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vader\n",
    "Article level sentiment analysis using lexical model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initialize Vader analyser\n",
    "analyser = SentimentIntensityAnalyzer()\n",
    "\n",
    "positive, negative, neutral, compound = ([] for i in range(4))\n",
    "\n",
    "## Calculate sentiment scores for each text\n",
    "for number, i in enumerate(texts):\n",
    "    \n",
    "    score = analyser.polarity_scores(i)\n",
    "    \n",
    "    positive.append(score['pos'])\n",
    "    neutral.append(score['neu'])\n",
    "    negative.append(score['neg'])\n",
    "    compound.append(score['compound'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Organize into df\n",
    "\n",
    "data = pd.DataFrame()\n",
    "\n",
    "data['texts'] = texts\n",
    "data['publications'] = publications\n",
    "data['dates'] = dates\n",
    "data['sent_comp'] = compound\n",
    "data['sent_pos'] = positive\n",
    "data['sent_neg'] = negative\n",
    "data['sent_neu'] = neutral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Washington Post        216\n",
       "Breitbart              165\n",
       "CNN                    113\n",
       "Business Insider        73\n",
       "Vox                     66\n",
       "National Review         59\n",
       "Atlantic                57\n",
       "New York Times          52\n",
       "NPR                     48\n",
       "Fox News                44\n",
       "Buzzfeed News           30\n",
       "Reuters                 27\n",
       "Talking Points Memo     22\n",
       "New York Post           16\n",
       "Guardian                12\n",
       "Name: publications, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.publications.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compound right-wing scores: \n",
      " Breitbart score:  16.817999999999998 \n",
      " Fox News Score:  3.6390000000000002 \n",
      " National Review Score:  6.5249999999999995\n",
      "\n",
      "Compound left-wing scores: \n",
      " CNN score:  10.788 \n",
      " NPR News Score:  4.496 \n",
      " NYT:  4.912\n"
     ]
    }
   ],
   "source": [
    "breitbart_score = data[data.publications == 'Breitbart'].sent_pos.sum()\n",
    "national_score = data[data.publications == 'National Review'].sent_pos.sum()\n",
    "fox_score = data[data.publications == 'Fox News'].sent_pos.sum()\n",
    "\n",
    "cnn_score = data[data.publications == 'CNN'].sent_pos.sum()\n",
    "npr_score = data[data.publications == 'NPR'].sent_pos.sum()\n",
    "nyt_score = data[data.publications == 'New York Times'].sent_pos.sum()\n",
    "\n",
    "print(\"Compound right-wing scores: \\n\",\n",
    "      \"Breitbart score: \", breitbart_score, \"\\n\",\n",
    "      \"Fox News Score: \", fox_score, \"\\n\",\n",
    "      \"National Review Score: \", national_score)\n",
    "\n",
    "print(\"\\nCompound left-wing scores: \\n\",\n",
    "      \"CNN score: \", cnn_score, \"\\n\",\n",
    "      \"NPR News Score: \", npr_score, \"\\n\",\n",
    "      \"NYT: \", nyt_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pseudocode\n",
    "\n",
    "    - Get sentences with relevant entity\n",
    "    - Use spacy dependecy parser to figure our phrases that refer to our entity\n",
    "    - Run sentiment analysis on these phrases, calculate list of sentiments for the entity per document\n",
    "    - Average the sentiment scores for that document\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful function calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fetch document by id\n",
    "es.get(index = 'corpus1', id=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Vader snippet test\n",
    "\n",
    "snippet = ' It amounts, instead, to a deliberate and cynical failure to implement the 2016 referendum result.'\n",
    "analyser.polarity_scores(snippet)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
